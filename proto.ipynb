{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.emoe import ElasticMoELlamaForCausalLM\n",
    "from models.emoe_config import ElasticMoELlamaConfig\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TinyLlama model to grab weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyllama_cfg = AutoConfig.from_pretrained(\n",
    "            \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", _from_pipeline=\"text-generation\", code_revision=None, revision=None, token=None, trust_remote_code=None, _commit_hash='77e23968eed12d195bd46c519aa679cc22a27ddc'\n",
    "        )\n",
    "\n",
    "# Replace 'model_name' with the actual model you're using, e.g., 'bert-base-uncased'\n",
    "tinyllama_model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test output\n",
    "\n",
    "The output of this cell is there to test we can get the same outputs with our new model, i.e. to ensure everything is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting?</s>\n",
      "<|assistant|>\n",
      "There is no definitive answer to this question as the number of helicopters that a human can eat in one sitting depends on various factors such as the size of the helicopter, the type of food, and the individual's appetite. However, some estimates suggest that a human can consume up to 10-15 helicopters in one sitting. This is based on the fact that a helicopter can carry a large amount of food and water, and the human body can process and digest large quantities of food quickly. However, it's always best to consult with a healthcare professional before consuming large quantities of food or drink in one sitting.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task = \"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Elastic MOE model and pull in weights from TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ElasticMoELlamaConfig(**tinyllama_cfg.to_dict())\n",
    "emoe_model = ElasticMoELlamaForCausalLM(config=cfg)\n",
    "emoe_model.load_state_dict(tinyllama_model.state_dict())\n",
    "emoe_model = emoe_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Inference\n",
    "\n",
    "Do greedy decoding without pipeline. Check against the output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobak/Documents/Work/ElasticMoE/ElasticMoE/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate \n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting? \n",
      "<|assistant|>\n",
      "There is no definitive answer to this question as the number of helicopters that a human can eat in one sitting depends on various factors such as the size of the helicopter, the type of food, and the individual's appetite. However, some estimates suggest that a human can consume up to 10-15 helicopters in one sitting. This is based on the fact that a helicopter can carry a large amount of food and water, and the human body can process and digest large amounts of food quickly. However, it is always recommended to consult with a healthcare professional before consuming large amounts of food or drink in one sitting.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"\"\"<|system|>\n",
    "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
    "<|user|>\n",
    "How many helicopters can a human eat in one sitting?</s>\n",
    "<|assistant|>\"\"\",]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Greedy decoding\n",
    "with torch.inference_mode():\n",
    "    outputs = emoe_model.generate(inputs[\"input_ids\"], max_length=1000, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "\n",
    "decoded_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "for response in decoded_responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Based Inference\n",
    "\n",
    "Can also add our model to a pipeline like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'ElasticMoELlamaForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting?</s>\n",
      "<|assistant|>\n",
      "There is no definitive answer to this question as the number of helicopters that a human can eat in one sitting depends on various factors such as the size of the helicopter, the type of food, and the individual's appetite. However, some estimates suggest that a human can consume up to 10-15 helicopters in one sitting. This is based on the fact that a helicopter can carry a large amount of food and water, and the human's appetite can be quite large. However, it's always best to consult with a medical professional or a nutritionist before attempting to consume a large number of helicopters in one sitting.\n"
     ]
    }
   ],
   "source": [
    "pipe2 = pipeline(task = \"text-generation\", model=emoe_model, tokenizer=tokenizer, config=cfg, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe2.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe2(prompt, max_new_tokens=256)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
